{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a955673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import accuracy, precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2694d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.util import align_tokens\n",
    "\n",
    "def represent_negation(tokens):\n",
    "    negation_words = [\"not\", \"n't\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"neither\", \"nor\"]\n",
    "    negation_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(negation_words))\n",
    "    twtokenizer = TweetTokenizer()\n",
    "    negation_tokens = []\n",
    "    is_negated = False\n",
    "    for token in tokens:\n",
    "        if re.match(negation_pattern, token, flags=re.IGNORECASE):\n",
    "            is_negated = not is_negated\n",
    "        else:\n",
    "            if is_negated:\n",
    "                negation_tokens.append(\"NOT_\" + token)\n",
    "            else:\n",
    "                negation_tokens.append(token)\n",
    "    return negation_tokens\n",
    "\n",
    "def processtweets(dirPath, limitStr):\n",
    "    # convert the limit argument from a string to an int\n",
    "    limit = int(limitStr)\n",
    "    # initialize NLTK built-in tweet tokenizer\n",
    "    twtokenizer = TweetTokenizer()\n",
    "\n",
    "    os.chdir(dirPath)\n",
    "\n",
    "    f = open('./corpus/downloaded-tweeti-b-dist.tsv', 'r')\n",
    "    # loop over lines in the file and use the first limit of them\n",
    "    # assuming that the tweets are sufficiently randomized\n",
    "    tweetdata = []\n",
    "    for line in f:\n",
    "        if len(tweetdata) < limit:\n",
    "            # remove final end of line character\n",
    "            line = line.strip()\n",
    "            # each line has 4 items separated by tabs\n",
    "            # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "            tweetdata.append(line.split('\\t')[2:4])\n",
    "\n",
    "    tweetdocs = []\n",
    "    # add all the tweets except the ones whose text is Not Available\n",
    "    for tweet in tweetdata:\n",
    "        if tweet[1] != 'Not Available':\n",
    "            # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "            tokens = twtokenizer.tokenize(tweet[1])\n",
    "\n",
    "            # Represent negation\n",
    "            tokens = represent_negation(tokens)\n",
    "\n",
    "            # Convert to lowercase\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "\n",
    "            # Remove punctuation\n",
    "            tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "            # Remove words with numbers (eg 11th)\n",
    "            tokens = [re.sub(r'\\S*\\d\\S*', '', token).strip() for token in tokens]\n",
    "            tokens = [token for token in tokens if token]\n",
    "\n",
    "            def decontracted(phrase):\n",
    "                # specific\n",
    "                phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "                phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "                # general\n",
    "                phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "                phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "                phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "                phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "                phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "                phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "                phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "                phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "                return phrase\n",
    "\n",
    "            tokens = [decontracted(token) for token in tokens]\n",
    "\n",
    "            # Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "\n",
    "            if tweet[0] == '\"positive\"':\n",
    "                label = 'pos'\n",
    "            elif tweet[0] == '\"negative\"':\n",
    "                label = 'neg'\n",
    "            elif tweet[0] in ('\"neutral\"', '\"objective\"', '\"objective-OR-neutral\"'):\n",
    "                label = 'neu'\n",
    "            else:\n",
    "                label = ''\n",
    "\n",
    "            tweetdocs.append((tokens, label))\n",
    "\n",
    "    df = pd.DataFrame(tweetdocs, columns=['tokens', 'labels'])\n",
    "    df.to_csv('tweetdocs1.csv', index=False)\n",
    "\n",
    "    def bag_of_words_features(tokens):\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        most_common_words = freq_dist.most_common(20)\n",
    "        word_features = {word: True for word, _ in most_common_words}\n",
    "        return word_features\n",
    "\n",
    "    feature_sets = []\n",
    "    for entry in tweetdocs:\n",
    "        tokens = entry[0]\n",
    "        label = entry[1]\n",
    "        features = bag_of_words_features(tokens)\n",
    "        feature_set = (features, label)\n",
    "        feature_sets.append(feature_set)\n",
    "\n",
    "    train_set = feature_sets[:7000]\n",
    "    test_set = feature_sets[7000:]\n",
    "\n",
    "    nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "    print('Accuracy:', nltk.classify.accuracy(nb_classifier, test_set))\n",
    "\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (features, label) in enumerate(test_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = nb_classifier.classify(features)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    prec = precision(refsets['pos'], testsets['pos'])\n",
    "    rec = recall(refsets['pos'], testsets['pos'])\n",
    "    f1 = f_measure(refsets['pos'], testsets['pos'])\n",
    "\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", rec)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a0885f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5844370860927153\n",
      "Precision: 0.6789667896678967\n",
      "Recall: 0.6865671641791045\n",
      "F1 Score: 0.6827458256029685\n"
     ]
    }
   ],
   "source": [
    "tweetdoc = processtweets('.',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
