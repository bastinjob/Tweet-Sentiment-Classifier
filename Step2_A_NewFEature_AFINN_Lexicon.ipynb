{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897035e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import accuracy, precision, recall, f_measure\n",
    "from nltk import pos_tag\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c767d0",
   "metadata": {},
   "source": [
    "### Step -1: AFINN Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b79ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read tweet training file, train and test a classifier \n",
    "def processtweets(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  # initialize NLTK built-in tweet tokenizer\n",
    "  twtokenizer = TweetTokenizer()\n",
    "  \n",
    "  os.chdir(dirPath)\n",
    "  \n",
    "  f = open('./downloaded-tweeti-b-dist.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  #    assuming that the tweets are sufficiently randomized\n",
    "  tweetdata = []\n",
    "  for line in f:\n",
    "    if (len(tweetdata) < limit):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "      tweetdata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  #for tweet in tweetdata[:10]:\n",
    "    #print (tweet)\n",
    "  \n",
    "  # create list of tweet documents as (list of words, label)\n",
    "  # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "  tweetdocs = []\n",
    "  # add all the tweets except the ones whose text is Not Available\n",
    "  for tweet in tweetdata:\n",
    "    if (tweet[1] != 'Not Available'):\n",
    "        \n",
    "      # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "      tokens = twtokenizer.tokenize(tweet[1])\n",
    "      \n",
    "      #Handling Emoticons\n",
    "      tokens = [re.sub(':\\)','happy', token) for token in tokens]\n",
    "      tokens = [re.sub(':\\(','sad', token) for token in tokens]\n",
    "        \n",
    "      #Removing URLS\n",
    "      tokens = [re.sub(r\"http\\S+\",'',token) for token in tokens]\n",
    "      \n",
    "      #Removing mentions\n",
    "      tokens = [re.sub(r\"@\\S+\", \"\", token) for token in tokens]\n",
    "    \n",
    "      # Convert to lowercase\n",
    "      tokens = [token.lower() for token in tokens]\n",
    "      \n",
    "      # Remove punctuation\n",
    "      tokens = [token for token in tokens if token not in string.punctuation]\n",
    "      \n",
    "      #Remove words with numbers (eg 11th)\n",
    "      tokens = [re.sub(r'\\S*\\d\\S*', '', token).strip() for token in tokens]\n",
    "      tokens = [token for token in tokens if token]\n",
    "    \n",
    "      def decontracted(phrase):\n",
    "    \n",
    "         # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "      \n",
    "      tokens = [decontracted(token) for token in tokens]\n",
    "    \n",
    "      # Remove stopwords\n",
    "      with open('stopwords_twitter.txt', 'r') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "      tokens = [token for token in tokens if token not in stop_words]\n",
    "      \n",
    "      # Lemmatization\n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "      \n",
    "      \n",
    "        \n",
    "      if tweet[0] == '\"positive\"':\n",
    "        label = 'pos'\n",
    "      else:\n",
    "        if tweet[0] == '\"negative\"':\n",
    "          label = 'neg'\n",
    "        else:\n",
    "          if (tweet[0] == '\"neutral\"') or (tweet[0] == '\"objective\"') or (tweet[0] == '\"objective-OR-neutral\"'):\n",
    "            label = 'neu'\n",
    "          else:\n",
    "            label = ''\n",
    "      tweetdocs.append((tokens, label))\n",
    "  \n",
    "  # print a few\n",
    " # for tweet in tweetdocs[:10]:\n",
    "    #print (tweet)\n",
    "  #Saving the cleaned data for EDA (one time procedure)\n",
    "  df = pd.DataFrame(tweetdocs, columns=['tokens','labels'])\n",
    "  df.to_csv('tweetdocs1.csv',index=False)\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------------#  \n",
    "  # continue as usual to get all words and create word features\n",
    "    \n",
    "\n",
    "  # feature sets from a feature definition function\n",
    "  \n",
    "   # Initialize the AFINN sentiment lexicon\n",
    "  afinn = Afinn()\n",
    "\n",
    "  # Function to generate AFINN sentiment lexicon features\n",
    "  def afinn_sentiment_features(tokens):\n",
    "    # Create a dictionary to store the AFINN sentiment scores\n",
    "    afinn_scores = {}\n",
    "    \n",
    "    # Calculate the AFINN sentiment score for each token\n",
    "    for token in tokens:\n",
    "        afinn_scores[token] = afinn.score(token)\n",
    "    \n",
    "    return afinn_scores\n",
    "\n",
    "  # List to store feature sets\n",
    "  feature_sets = []\n",
    "\n",
    "  # Iterate over each entry\n",
    "  for entry in tweetdocs:\n",
    "    tokens = entry[0]  # Extract tokens from the entry\n",
    "    label = entry[1]  # Extract label from the entry\n",
    "    \n",
    "    # Generate AFINN sentiment lexicon features for the tokens\n",
    "    features = afinn_sentiment_features(tokens)\n",
    "    \n",
    "    # Create a tuple of feature dictionary and label\n",
    "    feature_set = (features, label)\n",
    "    \n",
    "    # Add the feature set to the list\n",
    "    feature_sets.append(feature_set)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------#\n",
    "    # train and test a classifier\n",
    "  train_set = feature_sets[:8000]\n",
    "  test_set = feature_sets[8000:]\n",
    "    \n",
    "  nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "  print('Accuracy: ',nltk.classify.accuracy(nb_classifier,test_set))\n",
    "  \n",
    "  # Precision, Recall, and F1-score\n",
    "  refsets = collections.defaultdict(set)\n",
    "  testsets = collections.defaultdict(set)\n",
    "\n",
    "  for i, (features, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(features)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "  prec = precision(refsets['pos'], testsets['pos'])\n",
    "  rec = recall(refsets['pos'], testsets['pos'])\n",
    "  f1 = f_measure(refsets['pos'], testsets['pos'])\n",
    "\n",
    "  print(\"Precision:\", prec)\n",
    "  print(\"Recall:\", rec)\n",
    "  print(\"F1 Score:\", f1)\n",
    "    \n",
    "  # show most informative features'''      \n",
    "  print(nb_classifier.most_informative_features(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f16237e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5721153846153846\n",
      "Precision: 0.782608695652174\n",
      "Recall: 0.6206896551724138\n",
      "F1 Score: 0.6923076923076923\n",
      "[('fuck', -4.0), ('excited', 3.0), ('sorry', -1.0), ('amazing', 4.0), ('luck', 3.0), ('fun', 4.0), ('injury', -2.0), ('great', 3.0), ('thank', 2.0), ('sad', -2.0), ('awesome', 4.0), ('thanks', 2.0), ('missing', -2.0), ('anymore', 0.0), ('happy', 3.0), ('cant', 0.0), ('cry', -1.0), ('fucking', -4.0), ('bitch', -5.0), ('could not', 0.0), ('brilliant', 4.0), ('exciting', 3.0), ('suck', -3.0), ('interesting', 2.0), ('hate', -3.0)]\n"
     ]
    }
   ],
   "source": [
    "tweetdocs = processtweets('.',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15503667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install afinn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
