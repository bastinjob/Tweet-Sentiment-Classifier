{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7dbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.util import align_tokens\n",
    "from nltk import FreqDist, pos_tag\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import collections\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fe1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_negation(tokens):\n",
    "    negation_words = [\"not\", \"n't\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"neither\", \"nor\"]\n",
    "    negation_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(negation_words))\n",
    "    twtokenizer = TweetTokenizer()\n",
    "    negation_tokens = []\n",
    "    is_negated = False\n",
    "    for token in tokens:\n",
    "        if re.match(negation_pattern, token, flags=re.IGNORECASE):\n",
    "            is_negated = not is_negated\n",
    "        else:\n",
    "            if is_negated:\n",
    "                negation_tokens.append(\"NOT_\" + token)\n",
    "            else:\n",
    "                negation_tokens.append(token)\n",
    "    return negation_tokens\n",
    "\n",
    "def processtweets(dirPath, limitStr):\n",
    "    # convert the limit argument from a string to an int\n",
    "    limit = int(limitStr)\n",
    "    # initialize NLTK built-in tweet tokenizer\n",
    "    twtokenizer = TweetTokenizer()\n",
    "\n",
    "    os.chdir(dirPath)\n",
    "\n",
    "    f = open('./corpus/downloaded-tweeti-b-dist.tsv', 'r')\n",
    "    # loop over lines in the file and use the first limit of them\n",
    "    # assuming that the tweets are sufficiently randomized\n",
    "    tweetdata = []\n",
    "    for line in f:\n",
    "        if len(tweetdata) < limit:\n",
    "            # remove final end of line character\n",
    "            line = line.strip()\n",
    "            # each line has 4 items separated by tabs\n",
    "            # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "            tweetdata.append(line.split('\\t')[2:4])\n",
    "\n",
    "    tweetdocs = []\n",
    "    # add all the tweets except the ones whose text is Not Available\n",
    "    for tweet in tweetdata:\n",
    "        if tweet[1] != 'Not Available':\n",
    "            # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "            tokens = twtokenizer.tokenize(tweet[1])\n",
    "\n",
    "            # Represent negation\n",
    "            tokens = represent_negation(tokens)\n",
    "\n",
    "            # Convert to lowercase\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "\n",
    "            # Remove punctuation\n",
    "            tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "            # Remove words with numbers (eg 11th)\n",
    "            tokens = [re.sub(r'\\S*\\d\\S*', '', token).strip() for token in tokens]\n",
    "            tokens = [token for token in tokens if token]\n",
    "\n",
    "            def decontracted(phrase):\n",
    "                # specific\n",
    "                phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "                phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "                # general\n",
    "                phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "                phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "                phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "                phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "                phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "                phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "                phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "                phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "                return phrase\n",
    "\n",
    "            tokens = [decontracted(token) for token in tokens]\n",
    "\n",
    "            # Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "\n",
    "            if tweet[0] == '\"positive\"':\n",
    "                label = 'pos'\n",
    "            elif tweet[0] == '\"negative\"':\n",
    "                label = 'neg'\n",
    "            elif tweet[0] in ('\"neutral\"', '\"objective\"', '\"objective-OR-neutral\"'):\n",
    "                label = 'neu'\n",
    "            else:\n",
    "                label = ''\n",
    "\n",
    "            tweetdocs.append((tokens, label))\n",
    "\n",
    "    df = pd.DataFrame(tweetdocs, columns=['tokens', 'labels'])\n",
    "    df.to_csv('tweetdocs1.csv', index=False)\n",
    "\n",
    "    def bag_of_words_features(tokens):\n",
    "        # Unigrams\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        most_common_words = freq_dist.most_common(20)\n",
    "        word_features = {word: True for word, _ in most_common_words}\n",
    "\n",
    "        # Bigrams\n",
    "        bigrams = list(nltk.bigrams(tokens))\n",
    "        freq_dist_bigrams = FreqDist(bigrams)\n",
    "        most_common_bigrams = freq_dist_bigrams.most_common(10)\n",
    "        bigram_features = {bigram: True for bigram, _ in most_common_bigrams}\n",
    "\n",
    "        # POS tag counts\n",
    "        pos_tags = [tag for _, tag in pos_tag(tokens)]\n",
    "        freq_dist_pos = FreqDist(pos_tags)\n",
    "        most_common_pos = freq_dist_pos.most_common(10)\n",
    "        pos_features = {pos: True for pos, _ in most_common_pos}\n",
    "\n",
    "        # Sentiment word counts\n",
    "        sentiment_words = ['good', 'bad', 'happy', 'sad', 'great', 'awful']\n",
    "        sentiment_counts = {word: tokens.count(word) for word in sentiment_words}\n",
    "\n",
    "        feature_set = {**word_features, **bigram_features, **pos_features, **sentiment_counts}\n",
    "        return feature_set\n",
    "\n",
    "    feature_sets = []\n",
    "    for entry in tweetdocs:\n",
    "        tokens = entry[0]\n",
    "        label = entry[1]\n",
    "        features = bag_of_words_features(tokens)\n",
    "        feature_set = (features, label)\n",
    "        feature_sets.append(feature_set)\n",
    "\n",
    "    train_set = feature_sets[:7000]\n",
    "    test_set = feature_sets[7000:]\n",
    "\n",
    "    nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "    print('Accuracy:', nltk.classify.accuracy(nb_classifier, test_set))\n",
    "\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (features, label) in enumerate(test_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = nb_classifier.classify(features)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    prec = precision(refsets['pos'], testsets['pos'])\n",
    "    rec = recall(refsets['pos'], testsets['pos'])\n",
    "    f1 = f_measure(refsets['pos'], testsets['pos'])\n",
    "\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", rec)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53888195",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetdoc_merged = processtweets('.',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babebd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
