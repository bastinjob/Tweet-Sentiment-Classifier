{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897035e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import accuracy, precision, recall, f_measure\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4deed1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c767d0",
   "metadata": {},
   "source": [
    "### Step -1: POS tag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35b79ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read tweet training file, train and test a classifier \n",
    "def processtweets(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  # initialize NLTK built-in tweet tokenizer\n",
    "  twtokenizer = TweetTokenizer()\n",
    "  \n",
    "  os.chdir(dirPath)\n",
    "  \n",
    "  f = open('./downloaded-tweeti-b-dist.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  #    assuming that the tweets are sufficiently randomized\n",
    "  tweetdata = []\n",
    "  for line in f:\n",
    "    if (len(tweetdata) < limit):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "      tweetdata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  #for tweet in tweetdata[:10]:\n",
    "    #print (tweet)\n",
    "  \n",
    "  # create list of tweet documents as (list of words, label)\n",
    "  # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "  tweetdocs = []\n",
    "  # add all the tweets except the ones whose text is Not Available\n",
    "  for tweet in tweetdata:\n",
    "    if (tweet[1] != 'Not Available'):\n",
    "        \n",
    "      # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "      tokens = twtokenizer.tokenize(tweet[1])\n",
    "      \n",
    "      #Handling Emoticons\n",
    "      tokens = [re.sub(':\\)','happy', token) for token in tokens]\n",
    "      tokens = [re.sub(':\\(','sad', token) for token in tokens]\n",
    "        \n",
    "      #Removing URLS\n",
    "      tokens = [re.sub(r\"http\\S+\",'',token) for token in tokens]\n",
    "      \n",
    "      #Removing mentions\n",
    "      tokens = [re.sub(r\"@\\S+\", \"\", token) for token in tokens]\n",
    "    \n",
    "      # Convert to lowercase\n",
    "      tokens = [token.lower() for token in tokens]\n",
    "      \n",
    "      # Remove punctuation\n",
    "      tokens = [token for token in tokens if token not in string.punctuation]\n",
    "      \n",
    "      #Remove words with numbers (eg 11th)\n",
    "      tokens = [re.sub(r'\\S*\\d\\S*', '', token).strip() for token in tokens]\n",
    "      tokens = [token for token in tokens if token]\n",
    "    \n",
    "      def decontracted(phrase):\n",
    "    \n",
    "         # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "      \n",
    "      tokens = [decontracted(token) for token in tokens]\n",
    "    \n",
    "      # Remove stopwords\n",
    "      with open('stopwords_twitter.txt', 'r') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "      tokens = [token for token in tokens if token not in stop_words]\n",
    "      \n",
    "      # Lemmatization\n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "      \n",
    "      \n",
    "        \n",
    "      if tweet[0] == '\"positive\"':\n",
    "        label = 'pos'\n",
    "      else:\n",
    "        if tweet[0] == '\"negative\"':\n",
    "          label = 'neg'\n",
    "        else:\n",
    "          if (tweet[0] == '\"neutral\"') or (tweet[0] == '\"objective\"') or (tweet[0] == '\"objective-OR-neutral\"'):\n",
    "            label = 'neu'\n",
    "          else:\n",
    "            label = ''\n",
    "      tweetdocs.append((tokens, label))\n",
    "  \n",
    "  # print a few\n",
    " # for tweet in tweetdocs[:10]:\n",
    "    #print (tweet)\n",
    "  #Saving the cleaned data for EDA (one time procedure)\n",
    "  df = pd.DataFrame(tweetdocs, columns=['tokens','labels'])\n",
    "  df.to_csv('tweetdocs1.csv',index=False)\n",
    "\n",
    "  # possibly filter tokens \n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------------#  \n",
    "  # continue as usual to get all words and create word features\n",
    "    \n",
    "\n",
    "  # feature sets from a feature definition function\n",
    "  \n",
    "  # Function to generate POS tag features\n",
    "  def pos_tag_features(tokens):\n",
    "    # Perform POS tagging on the tokens\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Create a dictionary to store the POS tag features\n",
    "    pos_features = {}\n",
    "    \n",
    "    # Iterate over the POS tagged tokens\n",
    "    for token, tag in pos_tags:\n",
    "        pos_features[token] = tag\n",
    "    \n",
    "    return pos_features\n",
    "\n",
    "  # List to store feature sets\n",
    "  feature_sets = []\n",
    "\n",
    "  # Iterate over each entry\n",
    "  for entry in tweetdocs:\n",
    "    tokens = entry[0]  # Extract tokens from the entry\n",
    "    label = entry[1]  # Extract label from the entry\n",
    "    \n",
    "    # Generate POS tag features for the tokens\n",
    "    features = pos_tag_features(tokens)\n",
    "    \n",
    "    # Create a tuple of feature dictionary and label\n",
    "    feature_set = (features, label)\n",
    "    \n",
    "    # Add the feature set to the list\n",
    "    feature_sets.append(feature_set)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------#\n",
    "    # train and test a classifier\n",
    "  train_set = feature_sets[:7000]\n",
    "  test_set = feature_sets[7000:]\n",
    "    \n",
    "  nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "  print('Accuracy: ',nltk.classify.accuracy(nb_classifier,test_set))\n",
    "  \n",
    "  # Precision, Recall, and F1-score\n",
    "  refsets = collections.defaultdict(set)\n",
    "  testsets = collections.defaultdict(set)\n",
    "\n",
    "  for i, (features, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(features)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "  prec = precision(refsets['pos'], testsets['pos'])\n",
    "  rec = recall(refsets['pos'], testsets['pos'])\n",
    "  f1 = f_measure(refsets['pos'], testsets['pos'])\n",
    "\n",
    "  print(\"Precision:\", prec)\n",
    "  print(\"Recall:\", rec)\n",
    "  print(\"F1 Score:\", f1)\n",
    "    \n",
    "  # show most informative features'''      \n",
    "  print(nb_classifier.most_informative_features(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f16237e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5041390728476821\n",
      "Precision: 0.6930455635491607\n",
      "Recall: 0.539179104477612\n",
      "F1 Score: 0.6065057712486884\n",
      "[('excited', 'VBD'), ('fuck', 'NN'), ('sad', 'NN'), ('awesome', 'JJ'), ('luck', 'NN'), ('great', 'JJ'), ('sad', 'JJ'), ('injury', 'NN'), ('fun', 'NN'), ('excited', 'VBN'), ('wrong', 'JJ'), ('love', 'VB'), ('thank', 'NN'), ('thanks', 'NNS'), ('bitch', 'NN'), ('cool', 'JJ'), ('smh', 'NN'), ('sorry', 'NN'), ('happy', 'JJ'), ('amazing', 'JJ'), ('matter', 'NN'), ('anymore', 'RB'), ('sick', 'JJ'), ('fucking', 'VBG'), ('cry', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tweetdocs = processtweets('.',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa74ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
