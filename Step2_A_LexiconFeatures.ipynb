{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897035e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import accuracy, precision, recall, f_measure\n",
    "from nltk.corpus import subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c767d0",
   "metadata": {},
   "source": [
    "### Step -1: Subjectivity Lexicons Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b79ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read tweet training file, train and test a classifier \n",
    "def processtweets(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  # initialize NLTK built-in tweet tokenizer\n",
    "  twtokenizer = TweetTokenizer()\n",
    "  \n",
    "  os.chdir(dirPath)\n",
    "  \n",
    "  f = open('./downloaded-tweeti-b-dist.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  #    assuming that the tweets are sufficiently randomized\n",
    "  tweetdata = []\n",
    "  for line in f:\n",
    "    if (len(tweetdata) < limit):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "      tweetdata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  #for tweet in tweetdata[:10]:\n",
    "    #print (tweet)\n",
    "  \n",
    "  # create list of tweet documents as (list of words, label)\n",
    "  # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "  tweetdocs = []\n",
    "  # add all the tweets except the ones whose text is Not Available\n",
    "  for tweet in tweetdata:\n",
    "    if (tweet[1] != 'Not Available'):\n",
    "        \n",
    "      # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "      tokens = twtokenizer.tokenize(tweet[1])\n",
    "      \n",
    "      #Handling Emoticons\n",
    "      tokens = [re.sub(':\\)','happy', token) for token in tokens]\n",
    "      tokens = [re.sub(':\\(','sad', token) for token in tokens]\n",
    "        \n",
    "      #Removing URLS\n",
    "      tokens = [re.sub(r\"http\\S+\",'',token) for token in tokens]\n",
    "      \n",
    "      #Removing mentions\n",
    "      tokens = [re.sub(r\"@\\S+\", \"\", token) for token in tokens]\n",
    "    \n",
    "      # Convert to lowercase\n",
    "      tokens = [token.lower() for token in tokens]\n",
    "      \n",
    "      # Remove punctuation\n",
    "      tokens = [token for token in tokens if token not in string.punctuation]\n",
    "      \n",
    "      #Remove words with numbers (eg 11th)\n",
    "      tokens = [re.sub(r'\\S*\\d\\S*', '', token).strip() for token in tokens]\n",
    "      tokens = [token for token in tokens if token]\n",
    "    \n",
    "      def decontracted(phrase):\n",
    "    \n",
    "         # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "      \n",
    "      tokens = [decontracted(token) for token in tokens]\n",
    "    \n",
    "      # Remove stopwords\n",
    "      with open('stopwords_twitter.txt', 'r') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "      tokens = [token for token in tokens if token not in stop_words]\n",
    "      \n",
    "      # Lemmatization\n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "      \n",
    "      \n",
    "        \n",
    "      if tweet[0] == '\"positive\"':\n",
    "        label = 'pos'\n",
    "      else:\n",
    "        if tweet[0] == '\"negative\"':\n",
    "          label = 'neg'\n",
    "        else:\n",
    "          if (tweet[0] == '\"neutral\"') or (tweet[0] == '\"objective\"') or (tweet[0] == '\"objective-OR-neutral\"'):\n",
    "            label = 'neu'\n",
    "          else:\n",
    "            label = ''\n",
    "      tweetdocs.append((tokens, label))\n",
    "  \n",
    "  # print a few\n",
    " # for tweet in tweetdocs[:10]:\n",
    "    #print (tweet)\n",
    "  #Saving the cleaned data for EDA (one time procedure)\n",
    "  df = pd.DataFrame(tweetdocs, columns=['tokens','labels'])\n",
    "  df.to_csv('tweetdocs1.csv',index=False)\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------------#  \n",
    "  # continue as usual to get all words and create word features\n",
    "    \n",
    "\n",
    "  # feature sets from a feature definition function\n",
    "\n",
    "  # Function to load subjectivity lexicon from file\n",
    "  def load_subjectivity_lexicon(file_path):\n",
    "    lexicon = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('type=weaksubj') or line.startswith('type=strongsubj'):\n",
    "                fields = line.strip().split()\n",
    "                word = fields[2].split('=')[1]\n",
    "                polarity = fields[0].split('=')[1]\n",
    "                \n",
    "                lexicon[word] = polarity\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "  # Load subjectivity lexicon from file\n",
    "  lexicon_file = './subjclueslen1-HLTEMNLP05.tff'\n",
    "  subjectivity_lexicon = load_subjectivity_lexicon(lexicon_file)\n",
    "\n",
    "  # Function to generate subjectivity lexicon features\n",
    "  def subjectivity_lexicon_features(tokens):\n",
    "    # Create a dictionary to store the subjectivity labels\n",
    "    subjectivity_labels = {}\n",
    "    \n",
    "    # Iterate over the tokens\n",
    "    for token in tokens:\n",
    "        # Check if the token is present in the subjectivity lexicon\n",
    "        if token in subjectivity_lexicon:\n",
    "            # Assign the polarity label from the lexicon\n",
    "            polarity = subjectivity_lexicon[token]\n",
    "            \n",
    "            # Assign a subjectivity label based on polarity\n",
    "            subjectivity_labels[token] = True if polarity == 'positive' else False\n",
    "        else:\n",
    "            # Assign a subjectivity label of False for tokens not in the lexicon\n",
    "            subjectivity_labels[token] = False\n",
    "    \n",
    "    return subjectivity_labels\n",
    "\n",
    "  # List to store feature sets\n",
    "  feature_sets = []\n",
    "\n",
    "  # Iterate over each entry\n",
    "  for entry in tweetdocs:\n",
    "    tokens = entry[0]  # Extract tokens from the entry\n",
    "    label = entry[1]  # Extract label from the entry\n",
    "    \n",
    "    # Generate subjectivity lexicon features for the tokens\n",
    "    features = subjectivity_lexicon_features(tokens)\n",
    "    \n",
    "    # Create a tuple of feature dictionary and label\n",
    "    feature_set = (features, label)\n",
    "    \n",
    "    # Add the feature set to the list\n",
    "    feature_sets.append(feature_set)\n",
    "\n",
    "\n",
    "  print('length of feature set: ',len(feature_sets))\n",
    "#-----------------------------------------------------------------------------------------------------------------------#\n",
    "    # train and test a classifier\n",
    "  train_set = feature_sets[:7000]\n",
    "  test_set = feature_sets[7000:]\n",
    "    \n",
    "  nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "  print('Accuracy: ',nltk.classify.accuracy(nb_classifier,test_set))\n",
    "  \n",
    "  # Precision, Recall, and F1-score\n",
    "  refsets = collections.defaultdict(set)\n",
    "  testsets = collections.defaultdict(set)\n",
    "\n",
    "  for i, (features, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(features)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "  prec = precision(refsets['pos'], testsets['pos'])\n",
    "  rec = recall(refsets['pos'], testsets['pos'])\n",
    "  f1 = f_measure(refsets['pos'], testsets['pos'])\n",
    "\n",
    "  print(\"Precision:\", prec)\n",
    "  print(\"Recall:\", rec)\n",
    "  print(\"F1 Score:\", f1)\n",
    "    \n",
    "  # show most informative features'''\n",
    "  print(nb_classifier.most_informative_features(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f16237e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature set:  8208\n",
      "Accuracy:  0.5587748344370861\n",
      "Precision: 0.682\n",
      "Recall: 0.6361940298507462\n",
      "F1 Score: 0.6583011583011583\n",
      "[('fuck', False), ('excited', False), ('sorry', False), ('amazing', False), ('fun', False), ('luck', False), ('sad', False), ('great', False), ('injury', False), ('awesome', False), ('thank', False), ('fucking', False), ('cancelled', False), ('thanks', False), ('anymore', False), ('bitch', False), ('cant', False), ('happy', False), ('cry', False), ('suck', False), ('could not', False), ('matter', False), ('exciting', False), ('alone', False), ('missing', False)]\n"
     ]
    }
   ],
   "source": [
    "tweetdocs = processtweets('.',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9021bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
