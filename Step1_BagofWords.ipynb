{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "897035e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import accuracy, precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c767d0",
   "metadata": {},
   "source": [
    "### Step -1: Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35b79ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read tweet training file, train and test a classifier \n",
    "def processtweets(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  # initialize NLTK built-in tweet tokenizer\n",
    "  twtokenizer = TweetTokenizer()\n",
    "  \n",
    "  os.chdir(dirPath)\n",
    "  \n",
    "  f = open('./downloaded-tweeti-b-dist.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  #    assuming that the tweets are sufficiently randomized\n",
    "  tweetdata = []\n",
    "  for line in f:\n",
    "    if (len(tweetdata) < limit):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "      tweetdata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  #for tweet in tweetdata[:10]:\n",
    "    #print (tweet)\n",
    "  \n",
    "  # create list of tweet documents as (list of words, label)\n",
    "  # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "  tweetdocs = []\n",
    "  # add all the tweets except the ones whose text is Not Available\n",
    "  for tweet in tweetdata:\n",
    "    if (tweet[1] != 'Not Available'):\n",
    "        \n",
    "      # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "      tokens = twtokenizer.tokenize(tweet[1])\n",
    "      \n",
    "      #Handling Emoticons\n",
    "      tokens = [re.sub(':\\)','happy', token) for token in tokens]\n",
    "      tokens = [re.sub(':\\(','sad', token) for token in tokens]\n",
    "        \n",
    "      #Removing URLS\n",
    "      tokens = [re.sub(r\"http\\S+\",'',token) for token in tokens]\n",
    "      \n",
    "      #Removing mentions\n",
    "      tokens = [re.sub(r\"@\\S+\", \"\", token) for token in tokens]\n",
    "    \n",
    "      # Convert to lowercase\n",
    "      tokens = [token.lower() for token in tokens]\n",
    "      \n",
    "      # Remove punctuation\n",
    "      tokens = [token for token in tokens if token not in string.punctuation]\n",
    "      \n",
    "      #Remove words with numbers (eg 11th)\n",
    "      tokens = [re.sub(r'\\S*\\d\\S*', '', token).strip() for token in tokens]\n",
    "      tokens = [token for token in tokens if token]\n",
    "    \n",
    "      def decontracted(phrase):\n",
    "    \n",
    "         # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "      \n",
    "      tokens = [decontracted(token) for token in tokens]\n",
    "    \n",
    "      # Remove stopwords\n",
    "      # Read the stop words from the file\n",
    "      with open('stopwords_twitter.txt', 'r') as file:\n",
    "        stop_words = set(file.read().splitlines()) \n",
    "      tokens = [token for token in tokens if token not in stop_words]\n",
    "      \n",
    "      # Lemmatization\n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "      \n",
    "      \n",
    "        \n",
    "      if tweet[0] == '\"positive\"':\n",
    "        label = 'pos'\n",
    "      else:\n",
    "        if tweet[0] == '\"negative\"':\n",
    "          label = 'neg'\n",
    "        else:\n",
    "          if (tweet[0] == '\"neutral\"') or (tweet[0] == '\"objective\"') or (tweet[0] == '\"objective-OR-neutral\"'):\n",
    "            label = 'neu'\n",
    "          else:\n",
    "            label = ''\n",
    "      tweetdocs.append((tokens, label))\n",
    "  \n",
    "  # print a few\n",
    " # for tweet in tweetdocs[:10]:\n",
    "    #print (tweet)\n",
    "  #Saving the cleaned data for EDA (one time procedure)\n",
    "  df = pd.DataFrame(tweetdocs, columns=['tokens','labels'])\n",
    "  df.to_csv('tweetdocs1.csv',index=False)\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------------#  \n",
    "  # continue as usual to get all words and create word features\n",
    "    \n",
    "\n",
    "  # feature sets from a feature definition function\n",
    "  def bag_of_words_features(tokens):\n",
    "    # Calculate the frequency distribution of the tokens\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    \n",
    "    # Select the most frequent words as word features\n",
    "    most_common_words = freq_dist.most_common(20)  # Adjust the number as needed\n",
    "    \n",
    "    # Create a dictionary with word features and their presence/absence indicator\n",
    "    word_features = {word: True for word, _ in most_common_words}\n",
    "    \n",
    "    return word_features\n",
    "\n",
    "  # List to store feature sets\n",
    "  feature_sets = []\n",
    "\n",
    "# Iterate over each entry\n",
    "  for entry in tweetdocs:\n",
    "      tokens = entry[0]  # Extract tokens from the entry\n",
    "      label = entry[1]  # Extract label from the entry\n",
    "    \n",
    "      # Generate bag-of-words features for the tokens\n",
    "      features = bag_of_words_features(tokens)\n",
    "    \n",
    "      # Create a tuple of feature dictionary and label\n",
    "      feature_set = (features, label)\n",
    "    \n",
    "      # Add the feature set to the list\n",
    "      feature_sets.append(feature_set)\n",
    "  print('length of feature set: ',len(feature_sets))\n",
    "#-----------------------------------------------------------------------------------------------------------------------#\n",
    "    # train and test a classifier\n",
    "  train_set = feature_sets[:7000]\n",
    "  test_set = feature_sets[7000:]\n",
    "    \n",
    "  nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "  print('Accuracy: ',nltk.classify.accuracy(nb_classifier,test_set))\n",
    "  \n",
    "  # Precision, Recall, and F1-score\n",
    "  refsets = collections.defaultdict(set)\n",
    "  testsets = collections.defaultdict(set)\n",
    "\n",
    "  for i, (features, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(features)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "  prec = precision(refsets['pos'], testsets['pos'])\n",
    "  rec = recall(refsets['pos'], testsets['pos'])\n",
    "  f1 = f_measure(refsets['pos'], testsets['pos'])\n",
    "\n",
    "  print(\"Precision:\", prec)\n",
    "  print(\"Recall:\", rec)\n",
    "  print(\"F1 Score:\", f1)\n",
    "    \n",
    "  # show most informative features'''\n",
    "  print(nb_classifier.most_informative_features(25))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f16237e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature set:  8208\n",
      "Accuracy:  0.5587748344370861\n",
      "Precision: 0.682\n",
      "Recall: 0.6361940298507462\n",
      "F1 Score: 0.6583011583011583\n",
      "[('fuck', True), ('excited', True), ('sorry', True), ('amazing', True), ('fun', True), ('luck', True), ('sad', True), ('great', True), ('injury', True), ('awesome', True), ('thank', True), ('fucking', True), ('cancelled', True), ('thanks', True), ('anymore', True), ('bitch', True), ('cant', True), ('happy', True), ('cry', True), ('suck', True), ('could not', True), ('matter', True), ('exciting', True), ('alone', True), ('missing', True)]\n"
     ]
    }
   ],
   "source": [
    "tweetdocs = processtweets('.',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd87e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
